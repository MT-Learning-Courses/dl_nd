{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, I build a chatbot that you can converse with at the command line. The chatbot uses a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. It uses pretrained word embeddings to improve the performance of the model.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Downloads__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchdata\n",
      "  Downloading torchdata-0.5.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.6 MB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchdata) (2.23.0)\n",
      "Collecting torch==1.13.1\n",
      "  Downloading torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 887.5 MB 5.7 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting portalocker>=2.0.0\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.7/site-packages (from torchdata) (1.25.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata) (3.0.4)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[K     |████████████████████████████████| 849 kB 34.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\"\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 317.1 MB 23 kB/s s eta 0:00:01               | 71.7 MB 36.8 MB/s eta 0:00:07\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.13.1->torchdata) (3.7.4.1)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\"\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 557.1 MB 6.1 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\"\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.0 MB 36.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"->torch==1.13.1->torchdata) (0.34.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"->torch==1.13.1->torchdata) (45.2.0.post20200209)\n",
      "\u001b[31mERROR: torchvision 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.13.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: torchtext 0.12.0 has requirement torch==1.11.0, but you'll have torch 1.13.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, nvidia-cuda-nvrtc-cu11, torch, portalocker, torchdata\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 portalocker-2.7.0 torch-1.13.1 torchdata-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting Cython\n",
      "  Downloading Cython-0.29.33-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: Cython\n",
      "\u001b[33m  WARNING: The scripts cygdb, cython and cythonize are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed Cython-0.29.33\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "\u001b[31mERROR: torchvision 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.13.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: torchtext 0.12.0 has requirement torch==1.11.0, but you'll have torch 1.13.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: typing-extensions\n",
      "Successfully installed typing-extensions-4.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already up-to-date: torch in /root/.local/lib/python3.7/site-packages (1.13.1)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" in /root/.local/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" in /root/.local/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" in /root/.local/lib/python3.7/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" in /root/.local/lib/python3.7/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /root/.local/lib/python3.7/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"->torch) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"->torch) (45.2.0.post20200209)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchtext==0.9.0\n",
      "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext==0.9.0) (1.21.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.9.0) (4.43.0)\n",
      "Collecting torch==1.8.0\n",
      "  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 735.5 MB 9.0 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext==0.9.0) (2.23.0)\n",
      "Requirement already satisfied: typing-extensions in /root/.local/lib/python3.7/site-packages (from torch==1.8.0->torchtext==0.9.0) (4.4.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (1.25.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (2019.11.28)\n",
      "\u001b[31mERROR: torchdata 0.5.1 has requirement torch==1.13.1, but you'll have torch 1.8.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: torchvision 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.8.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: torch, torchtext\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.13.1\n",
      "    Uninstalling torch-1.13.1:\n",
      "      Successfully uninstalled torch-1.13.1\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx and convert-onnx-to-caffe2 are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed torch-1.8.0 torchtext-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchdata\n",
    "%pip install Cython\n",
    "%pip install typing-extensions --upgrade\n",
    "%pip install torch --upgrade\n",
    "%pip install torchtext==0.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Restart the kernel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "import torchtext\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import gensim.downloader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset download\n",
    "train, test = torchtext.datasets.SQuAD1(\"root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Squad 1 dataset contains 100k+ questions and answers based on over 500 articles. More information can be found [here](https://rajpurkar.github.io/SQuAD-explorer/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data rows: 87599\n",
      "Number of test data rows: 10570\n"
     ]
    }
   ],
   "source": [
    "# check number of rows\n",
    "print(f\"Number of training data rows: {train.num_lines}\")\n",
    "print(f\"Number of test data rows: {test.num_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Answer: Saint Bernadette Soubirous\n"
     ]
    }
   ],
   "source": [
    "# print out example question\n",
    "for context, question, answer, answer_start in train:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer[0]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data into dataframe\n",
    "def convert_to_df(iterator):\n",
    "\n",
    "    questions, answers = [], []\n",
    "\n",
    "    for line in iterator:\n",
    "        _, question, answer, _ = line\n",
    "\n",
    "\n",
    "        questions.append(question)\n",
    "        answers.append(answer[0])\n",
    "\n",
    "    data_dict = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_train = convert_to_df(train)\n",
    "df_test = convert_to_df(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Data Checks_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 87598 entries, 0 to 87597\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   question  87598 non-null  object\n",
      " 1   answer    87598 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79532</th>\n",
       "      <td>How many tourist visas were granted in 2003?</td>\n",
       "      <td>300,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39645</th>\n",
       "      <td>Which CBS correspondant graduated from The Med...</td>\n",
       "      <td>Richard Threlkeld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68742</th>\n",
       "      <td>Name the main airport?</td>\n",
       "      <td>Philadelphia International Airport (PHL)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11391</th>\n",
       "      <td>What year did Terminator Genisys debut?</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33264</th>\n",
       "      <td>Which town was the site of General von Steinme...</td>\n",
       "      <td>Spicheren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17007</th>\n",
       "      <td>What lines comprise the Beijing-Shanghai (Jing...</td>\n",
       "      <td>the old Jinpu and Huning Railways</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35625</th>\n",
       "      <td>In what year was Athanasius consecrated?</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49194</th>\n",
       "      <td>What were the followers of Jan Hus called?</td>\n",
       "      <td>Hussites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28425</th>\n",
       "      <td>What team did Barcelona beat to win La Liga fo...</td>\n",
       "      <td>Real Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7003</th>\n",
       "      <td>For how many seasons were the three original j...</td>\n",
       "      <td>eight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question  \\\n",
       "79532       How many tourist visas were granted in 2003?   \n",
       "39645  Which CBS correspondant graduated from The Med...   \n",
       "68742                             Name the main airport?   \n",
       "11391            What year did Terminator Genisys debut?   \n",
       "33264  Which town was the site of General von Steinme...   \n",
       "17007  What lines comprise the Beijing-Shanghai (Jing...   \n",
       "35625           In what year was Athanasius consecrated?   \n",
       "49194         What were the followers of Jan Hus called?   \n",
       "28425  What team did Barcelona beat to win La Liga fo...   \n",
       "7003   For how many seasons were the three original j...   \n",
       "\n",
       "                                         answer  \n",
       "79532                                   300,000  \n",
       "39645                         Richard Threlkeld  \n",
       "68742  Philadelphia International Airport (PHL)  \n",
       "11391                                      2015  \n",
       "33264                                 Spicheren  \n",
       "17007         the old Jinpu and Huning Railways  \n",
       "35625                                       326  \n",
       "49194                                  Hussites  \n",
       "28425                               Real Madrid  \n",
       "7003                                      eight  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a feel for what the data looks like\n",
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Build a Vocab__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\" This vocabulary class cleans and indexes words.\n",
    "    \"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"<SOS>\": SOS_token, \"<EOS>\": EOS_token}\n",
    "        self.index2word = {SOS_token: \"<SOS>\", EOS_token: \"<EOS>\"}\n",
    "        self.word2count = {}\n",
    "        self.n_words = 2 # count SOS and EOS\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "    \n",
    "    # Index words in our vocabulary\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index: # if word not in index\n",
    "            self.word2index[word] = self.n_words # add word and word no to words dictionary\n",
    "            self.word2count[word] = 1 # initialise word count\n",
    "            self.index2word[self.n_words] = word # add word to index\n",
    "            self.n_words +=1\n",
    "        else:\n",
    "            self.word2count[word] += 1 # increment word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = s.lower().split()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def prepare_text(sentence):\n",
    "    sentence = ''.join([ch.lower() for ch in sentence if ch not in string.punctuation])\n",
    "    sentence = ' '.join(stemmer.stem(w) for w in sentence.split())\n",
    "    tokens = nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(sentence)\n",
    "    return tokens\n",
    "\n",
    "df_train[\"question\"] = df_train[\"question\"].apply(prepare_text)\n",
    "df_train[\"answer\"] = df_train[\"answer\"].apply(prepare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 87598 pairs\n",
      "filtered to 40716 pairs\n",
      "final number of pairs: 2500\n"
     ]
    }
   ],
   "source": [
    "def getPairs(df):\n",
    "    # convert df to list of pairs\n",
    "    temp1 = df['question'].apply(lambda x: \" \".join(x) ).to_list()\n",
    "    temp2 = df['answer'].apply(lambda x: \" \".join(x) ).to_list()\n",
    "    return [list(i) for i in zip(temp1, temp2)]\n",
    "\n",
    "pairs = getPairs(df_train)\n",
    "print(f\"read {len(pairs)} pairs\")\n",
    "\n",
    "# filter to shorter pairs\n",
    "MAX_LENGTH = 10\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "pairs = filterPairs(pairs)\n",
    "print(f\"filtered to {len(pairs)} pairs\")\n",
    "\n",
    "# reduce dataset size to reduce training time\n",
    "dataset_size = 2500\n",
    "pairs = pairs[:dataset_size]\n",
    "print(f\"final number of pairs: {len(pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what is a cultur phenomenon', 'art']\n"
     ]
    }
   ],
   "source": [
    "# print random sentence and answer\n",
    "random_pair = random.choice(pairs)\n",
    "print(random_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 2816 question words\n",
      "Added 2655 answer words\n"
     ]
    }
   ],
   "source": [
    "# create and populate vocab object\n",
    "vocab_questions = Vocab(\"questions\")\n",
    "vocab_answers = Vocab(\"answers\")\n",
    "for question, answer in pairs:\n",
    "    vocab_questions.addSentence(question)\n",
    "    vocab_answers.addSentence(answer)\n",
    "    \n",
    "# check words added\n",
    "print(f\"Added {vocab_questions.n_words} question words\")\n",
    "print(f\"Added {vocab_answers.n_words} answer words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what new york thoroughfar is museum mile locat on', 'fifth avenu']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[   2],\n",
       "         [ 451],\n",
       "         [2236],\n",
       "         [2466],\n",
       "         [   3],\n",
       "         [ 140],\n",
       "         [2300],\n",
       "         [ 274],\n",
       "         [  79],\n",
       "         [   1]], device='cuda:0'),\n",
       " tensor([[1528],\n",
       "         [2240],\n",
       "         [   1]], device='cuda:0'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def indexesFromSentence(sentence, vocab):\n",
    "    # returns a list of indices representing the input sentence\n",
    "    return [vocab.word2index[word] for word in sentence.split(\" \")]\n",
    "\n",
    "def tensorFromSentence(sentence, vocab):\n",
    "    # appends a EOS token and returns a tensor list of indices representing the input sentence\n",
    "    indexes = indexesFromSentence(sentence, vocab)\n",
    "    indexes.append(EOS_token)\n",
    "    # view(-1, 1) specifies that we want the shape of 1 column and whatever number of rows to achieve that shape\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    # combines the function above and returns question and answer tensors\n",
    "    question_tensor = tensorFromSentence(pair[0], vocab_questions)\n",
    "    answer_tensor = tensorFromSentence(pair[1], vocab_answers)\n",
    "    return (question_tensor, answer_tensor)\n",
    "\n",
    "# test functions\n",
    "random_pair = random.choice(pairs)\n",
    "print(random_pair)\n",
    "tensorsFromPair(random_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Download the gensim embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "glove_vectors = gensim.downloader.load(\"glove-wiki-gigaword-300\")\n",
    "\n",
    "embedding_dim = glove_vectors.vectors.shape[1]\n",
    "print(embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Explore embedding model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6998663544654846),\n",
       " ('person', 0.6443442106246948),\n",
       " ('boy', 0.6208277940750122),\n",
       " ('he', 0.5926738381385803),\n",
       " ('men', 0.5819568634033203),\n",
       " ('himself', 0.5810033082962036),\n",
       " ('one', 0.5779521465301514),\n",
       " ('another', 0.5721587538719177),\n",
       " ('who', 0.5703631639480591),\n",
       " ('him', 0.5670831203460693)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.29784  , -0.13255  , -0.14505  , -0.22752  , -0.027429 ,\n",
       "        0.11005  , -0.039245 , -0.0089607, -0.18866  , -1.1213   ,\n",
       "        0.34793  , -0.30056  , -0.50103  , -0.031383 , -0.032185 ,\n",
       "        0.018318 , -0.090429 , -0.14427  , -0.14306  , -0.057477 ,\n",
       "       -0.020931 ,  0.56276  , -0.018557 ,  0.15168  , -0.25586  ,\n",
       "       -0.081564 ,  0.2803   , -0.10585  , -0.16777  ,  0.21814  ,\n",
       "       -0.11845  ,  0.56475  , -0.12645  , -0.062461 , -0.68043  ,\n",
       "        0.10507  ,  0.24793  , -0.20249  , -0.30726  ,  0.42815  ,\n",
       "        0.38378  , -0.19371  , -0.075951 , -0.058287 , -0.067195 ,\n",
       "        0.2192   ,  0.56116  , -0.28156  , -0.13705  ,  0.45754  ,\n",
       "       -0.14671  , -0.18562  , -0.074146 ,  0.60737  ,  0.07952  ,\n",
       "        0.41023  ,  0.18377  , -0.08532  ,  0.43795  , -0.34727  ,\n",
       "        0.2077   ,  0.50454  ,  0.40244  ,  0.1095   , -0.48078  ,\n",
       "       -0.22372  , -0.54619  , -0.20782  ,  0.13751  , -0.16206  ,\n",
       "       -0.24835  ,  0.17124  ,  0.037355 ,  0.14547  , -0.056205 ,\n",
       "        0.2644   , -0.38029  ,  0.0029947,  0.051608 ,  0.21807  ,\n",
       "       -0.071087 ,  0.31543  ,  0.24593  ,  0.34444  , -0.36354  ,\n",
       "       -0.12137  , -0.013511 ,  0.049564 ,  0.031386 ,  0.10316  ,\n",
       "       -0.44301  ,  0.11663  , -0.055576 , -0.091434 , -0.070799 ,\n",
       "       -0.017031 ,  0.046024 ,  0.096063 ,  0.10148  , -0.33467  ,\n",
       "       -0.043516 ,  0.38271  ,  0.018411 ,  0.0049479,  0.35587  ,\n",
       "        0.40432  ,  0.4823   ,  0.12416  , -0.051926 , -0.081769 ,\n",
       "       -0.21542  ,  0.71766  , -0.0052553, -0.55428  ,  0.68033  ,\n",
       "        0.20255  , -0.19927  , -0.11563  ,  0.0030103, -0.74399  ,\n",
       "       -0.21086  ,  0.069584 , -0.20315  , -0.17027  , -0.11292  ,\n",
       "       -0.044594 , -0.2145   ,  0.26097  ,  0.36969  , -0.65155  ,\n",
       "        0.048649 ,  0.044304 , -0.26604  ,  0.23976  , -0.38281  ,\n",
       "       -0.02353  , -0.020519 , -0.52202  , -0.30595  ,  0.088525 ,\n",
       "        0.28395  ,  0.15056  ,  0.23202  ,  0.30538  , -0.53674  ,\n",
       "        0.21362  ,  0.10999  ,  0.4685   , -0.60017  , -0.13141  ,\n",
       "        0.18707  ,  0.11636  ,  0.24703  ,  0.17168  ,  0.40658  ,\n",
       "       -0.19039  ,  0.033278 , -0.26374  ,  0.28722  ,  0.27659  ,\n",
       "        0.30153  , -0.12622  ,  0.5225   ,  0.13887  , -0.15012  ,\n",
       "       -0.0038264, -0.074287 ,  0.46348  ,  0.26215  , -0.62671  ,\n",
       "        0.21841  , -0.26531  , -0.75294  , -0.36826  ,  0.072989 ,\n",
       "       -0.071807 ,  0.10612  , -0.087064 , -0.053871 ,  0.22379  ,\n",
       "        0.38538  ,  0.053972 ,  0.21208  ,  0.39662  , -0.38609  ,\n",
       "       -0.53     , -0.34169  ,  0.067624 ,  0.11835  ,  0.60372  ,\n",
       "        0.21744  , -0.12806  ,  0.13802  ,  0.22463  , -0.21566  ,\n",
       "       -0.25167  , -0.44859  , -0.39111  , -0.5259   ,  0.21266  ,\n",
       "        1.9729   ,  0.13851  ,  0.32966  ,  0.047626 , -0.37787  ,\n",
       "       -0.31769  , -0.1283   ,  0.34396  , -0.11061  , -0.23346  ,\n",
       "       -0.45134  ,  0.1605   , -0.32801  ,  0.033373 ,  0.42571  ,\n",
       "        0.42283  , -0.16932  ,  0.14068  ,  0.14087  ,  0.20856  ,\n",
       "       -0.24912  ,  0.38337  ,  0.43722  , -0.090575 ,  0.021208 ,\n",
       "       -0.22334  , -0.42596  , -0.47252  , -0.1995   ,  0.32459  ,\n",
       "        0.39765  , -0.091573 , -0.073528 , -0.20074  ,  0.22083  ,\n",
       "        0.011314 ,  0.11977  , -0.14888  , -0.27152  ,  0.28908  ,\n",
       "        0.20407  ,  0.098377 , -0.1424   , -0.059699 ,  0.032529 ,\n",
       "       -0.1877   ,  0.24643  , -0.1128   ,  0.14666  ,  0.076001 ,\n",
       "        0.31109  , -0.22119  ,  0.033051 ,  0.066731 ,  1.5449   ,\n",
       "       -0.11096  ,  0.26438  ,  0.1791   ,  0.039628 , -0.46497  ,\n",
       "        0.40052  , -0.057004 ,  0.11391  ,  0.42397  , -0.57411  ,\n",
       "        0.17205  ,  0.44401  , -0.030134 , -0.22061  ,  0.47222  ,\n",
       "       -0.12734  , -0.12288  ,  0.33832  ,  0.41698  ,  0.18355  ,\n",
       "        0.093473 , -1.9373   ,  0.053236 , -0.10361  ,  0.056681 ,\n",
       "       -0.12726  , -0.11681  ,  0.55039  ,  0.33644  ,  0.27706  ,\n",
       "        0.71184  , -0.1304   ,  0.26108  , -0.2481   ,  0.0032735,\n",
       "       -0.16505  ,  0.11827  , -0.26322  , -0.10169  ,  0.24275  ,\n",
       "        0.16927  , -0.15809  ,  0.27613  , -0.2832   ,  0.50554  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors[\"man\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Create weights matrix to map each word embedding to the word in the vocab_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_weight_matrix(vocab, embed_dim=embedding_dim, glove=glove_vectors):\n",
    "\n",
    "    matrix_len = len(vocab.word2index) # length of vocab\n",
    "    weights_matrix = np.zeros((matrix_len, embed_dim)) # initialise empty weights matrix\n",
    "\n",
    "    words_found = 0\n",
    "    # add words to weights matrix\n",
    "    for i, word in enumerate(vocab.word2index):\n",
    "        try:\n",
    "            weights_matrix[i] = glove[word] # map to glove vector word embedding\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim, )) # insert random weights\n",
    "            \n",
    "    return weights_matrix\n",
    "            \n",
    "vocab_questions_matrix = create_vocab_weight_matrix(vocab_questions)\n",
    "vocab_answers_matrix = create_vocab_weight_matrix(vocab_answers)\n",
    "\n",
    "# check\n",
    "print(vocab_questions_matrix[vocab_questions.word2index[\"who\"]] - glove_vectors[\"who\"])\n",
    "print(vocab_answers_matrix[vocab_answers.word2index[\"the\"]] - glove_vectors[\"the\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define Model Structure__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oQLTP2Wmi1eB"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        # copy loaded weights matrix into embedding weights - a vector representation of the model word inputs\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(vocab_questions_matrix))\n",
    "        # initialise lstm to take input dimension of embedding size and output hidden dimension\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers=num_layers)\n",
    "        \n",
    "    \n",
    "    def forward(self, src, hidden, cell_state):\n",
    "        \n",
    "        '''\n",
    "        Inputs: src, the src vector\n",
    "        Outputs: output, the encoder outputs\n",
    "                hidden, the hidden state\n",
    "                cell, the cell state\n",
    "        '''\n",
    "        # src shape: (seq_length, batch_size)\n",
    "        embedding = self.embedding(src).view(1, 1, -1)\n",
    "        # embedding shape: (seq_length, batch_size, embedding_size)\n",
    "        output, (hidden, cell) = self.lstm(embedding, (hidden, cell_state)) # output, hidden and cell state\n",
    "        \n",
    "        return output, hidden, cell\n",
    "    \n",
    "    def initHidden(self):# initialise zero tensor with shape (1, 1, hidden_size)\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "      \n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size # length of vocab\n",
    "        self.num_layers = num_layers        \n",
    "        \n",
    "        # self.embedding provides a vector representation of the target to our model\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        # self.embedding.weight.data.copy_(torch.from_numpy(vocab_answers_matrix))\n",
    "        # self.lstm, accepts the embeddings and outputs a hidden state\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers=num_layers)\n",
    "        # self.output, predicts on the hidden state via a linear output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, trg, hidden, cell):\n",
    "        \n",
    "        '''\n",
    "        Inputs: trg, the target vector\n",
    "        Outputs: predictions, the predictions\n",
    "                hidden, the hidden state\n",
    "                cell, the cell state\n",
    "        '''\n",
    "        # shape of src is batch_size, but we want (1, batch_size)\n",
    "        # we want batch_size batches of a single word at a time\n",
    "        trg = trg.unsqueeze(0) # add one dimension\n",
    "        embedding = self.embedding(trg).view(1, 1, -1)\n",
    "        # embedding shape: (1, batch_size, embedding_size)\n",
    "        output, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
    "        # shape of outputs: (1, batch_size, hidden_size)\n",
    "        predictions = self.softmax(self.fc(output[0]))\n",
    "        # shape of predictions: (1, batch_size, length_of_vocab)        \n",
    "        \n",
    "        return predictions, hidden, cell\n",
    "    \n",
    "    def initHidden(self): # initialise zero tensor with shape (1, 1, hidden_size)\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "        \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device=device\n",
    "    \n",
    "    # teacher forcing ratio - switch between using target and prediction words as inputs for next word\n",
    "    def forward(self, src, trg, max_trg_len=100, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = src.shape[1]\n",
    "        \n",
    "        input_length = src.size(0) # how many question words\n",
    "        target_length = trg.size(0) if self.training else max_trg_len# how many answer words\n",
    "        \n",
    "        target_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        encoder_hidden = torch.zeros([self.encoder.num_layers, 1, self.encoder.hidden_size]).to(device)\n",
    "        cell = torch.zeros([self.encoder.num_layers, 1, self.encoder.hidden_size]).to(device)\n",
    "        outputs = torch.zeros(target_length, batch_size, target_vocab_size).to(self.device)\n",
    "        \n",
    "        for i in range(input_length):\n",
    "            # process and encode the entire question\n",
    "            _, encoder_hidden, cell = self.encoder(src[i], encoder_hidden, cell)\n",
    "            \n",
    "        decoder_hidden = encoder_hidden # initialise decoder hidden state with encoder hidden state\n",
    "        decoder_input = torch.tensor([[SOS_token]]).to(device) # add SOS token as first predicted word\n",
    "        \n",
    "        for t in range(1, target_length):\n",
    "            output, decoder_hidden, cell = self.decoder(decoder_input, decoder_hidden, cell)\n",
    "            \n",
    "            outputs[t] = output # add decoder predictions array to outputs\n",
    "            # (batch_size, vocab_size)\n",
    "            best_guess = output.argmax(1) # get index of best word guess\n",
    "            teacher_force = False # initialise\n",
    "            if self.training:\n",
    "                # use target word if teacher forcing, else use word with the highest predicted value\n",
    "                teacher_force = random.random() < teacher_forcing_ratio # update where relevant\n",
    "                decoder_input = trg[t] if teacher_force else best_guess \n",
    "            else:\n",
    "                decoder_input = best_guess\n",
    "                \n",
    "            if (teacher_force == False and decoder_input.item() == EOS_token):\n",
    "                break\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def showPlot(points):\n",
    "    # This function plots the input points and sets defined tick intervals on the y axis\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    loc = ticker.MultipleLocator(base=0.2) # define tick intervals\n",
    "    ax.yaxis.set_major_locator(loc) # set y axis tick intervals\n",
    "    plt.plot(points)\n",
    "    \n",
    "def asMinutes(s):\n",
    "    # format seconds as minutes\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    # calculate the time between now and since\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (- %s)\" % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcModel(model, input_tensor, target_tensor, model_optimizer, criterion):\n",
    "    model_optimizer.zero_grad() # don't accumulate gradient\n",
    "    \n",
    "    loss = 0\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    output = model(input_tensor, target_tensor)\n",
    "    \n",
    "    num_iter = output.size(0) # number of predicted words\n",
    "    \n",
    "    # calculate loss from predicted sentence with expected result\n",
    "    for ot in range(num_iter):\n",
    "        loss += criterion(output[ot], target_tensor[ot])\n",
    "    \n",
    "    loss.backward() # calculate gradients\n",
    "    model_optimizer.step() # update weights\n",
    "    epoch_loss = loss.item() / num_iter # avg loss\n",
    "    \n",
    "    return epoch_loss\n",
    "\n",
    "def trainModel(model, pairs, epochs, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    \n",
    "    criterion = nn.NLLLoss() # crossentropyloss = softmax + NLLLoss\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0 # reset every print_every\n",
    "    plot_loss_total = 0 # reset every plot_every\n",
    "    \n",
    "    n_pairs = len(pairs)\n",
    "    num_iterations= epochs * n_pairs\n",
    "    training_pairs = [tensorsFromPair(pairs[i]) for i in range(n_pairs)]\n",
    "    \n",
    "    end_training = False\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for iter_ in range(1, n_pairs+1):\n",
    "            training_pair = training_pairs[iter_ - 1]\n",
    "            input_tensor = training_pair[0]\n",
    "            target_tensor = training_pair[1]\n",
    "\n",
    "            loss = calcModel(model, input_tensor, target_tensor, optimizer, criterion)\n",
    "\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss        \n",
    "\n",
    "            if iter_ % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print(\"{0:s} ({1:d} {2:.0f}%), {3:.4f}\". format(timeSince(start, iter_*epoch / num_iterations),\n",
    "                                                     iter_*epoch, iter_*epoch / num_iterations * 100, print_loss_avg))\n",
    "                if print_loss_avg < 0.01:\n",
    "                    print(\"Stopped training early, as loss is sufficiently low\")\n",
    "                    end_training = True\n",
    "                    break\n",
    "\n",
    "            if iter_ % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "        \n",
    "        if end_training:\n",
    "            break\n",
    "            \n",
    "    showPlot(plot_losses)\n",
    "            \n",
    "    torch.save(model.state_dict(), \"mytraining.pt\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(2816, 300)\n",
       "    (lstm): LSTM(300, 1024)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(2655, 300)\n",
       "    (lstm): LSTM(300, 1024)\n",
       "    (fc): Linear(in_features=1024, out_features=2655, bias=True)\n",
       "    (softmax): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "input_size_enc = len(vocab_questions.word2index)\n",
    "input_size_dec = len(vocab_answers.word2index)\n",
    "output_size = len(vocab_answers.word2index)\n",
    "enc_embed_size = embedding_dim\n",
    "dec_embed_size = embedding_dim\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 40\n",
    "num_iterations = dataset_size * epochs\n",
    "\n",
    "# initialise models\n",
    "encoder = Encoder(input_size_enc, enc_embed_size, hidden_size, num_layers=num_layers)\n",
    "decoder = Decoder(input_size_dec, dec_embed_size, hidden_size, output_size, num_layers=num_layers)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 10s (- 46m 7s) (2500 2%), 2.0401\n",
      "2m 19s (- 44m 18s) (5000 5%), 1.9206\n",
      "3m 24s (- 42m 0s) (7500 8%), 1.8512\n",
      "4m 29s (- 40m 24s) (10000 10%), 1.7742\n",
      "5m 35s (- 39m 5s) (12500 12%), 1.6581\n",
      "6m 41s (- 37m 53s) (15000 15%), 1.5609\n",
      "7m 48s (- 36m 47s) (17500 18%), 1.4534\n",
      "8m 55s (- 35m 43s) (20000 20%), 1.2870\n",
      "10m 3s (- 34m 40s) (22500 22%), 1.1477\n",
      "11m 12s (- 33m 37s) (25000 25%), 0.9802\n",
      "12m 21s (- 32m 34s) (27500 28%), 0.8386\n",
      "13m 30s (- 31m 31s) (30000 30%), 0.6881\n",
      "14m 40s (- 30m 28s) (32500 32%), 0.5435\n",
      "15m 50s (- 29m 24s) (35000 35%), 0.4363\n",
      "17m 0s (- 28m 20s) (37500 38%), 0.3486\n",
      "18m 10s (- 27m 15s) (40000 40%), 0.2645\n",
      "19m 20s (- 26m 10s) (42500 42%), 0.1932\n",
      "20m 31s (- 25m 4s) (45000 45%), 0.1052\n",
      "21m 41s (- 23m 59s) (47500 48%), 0.0738\n",
      "22m 52s (- 22m 52s) (50000 50%), 0.0464\n",
      "24m 3s (- 21m 46s) (52500 52%), 0.0295\n",
      "25m 14s (- 20m 39s) (55000 55%), 0.0202\n",
      "26m 25s (- 19m 31s) (57500 57%), 0.0141\n",
      "27m 36s (- 18m 24s) (60000 60%), 0.0148\n",
      "28m 47s (- 17m 16s) (62500 62%), 0.0114\n",
      "29m 58s (- 16m 8s) (65000 65%), 0.0114\n",
      "31m 9s (- 15m 0s) (67500 68%), 0.0120\n",
      "32m 20s (- 13m 51s) (70000 70%), 0.0086\n",
      "Stopped training early, as loss is sufficiently low\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train model\n",
    "model_1 = trainModel(model, \n",
    "                     pairs, \n",
    "                     learning_rate=learning_rate, \n",
    "                     epochs=epochs, \n",
    "                     print_every=dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, source, max_trg_len=100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(source, vocab_questions)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        \n",
    "        decoded_words = []\n",
    "        \n",
    "        output = model(input_tensor, None)\n",
    "        \n",
    "        for ot in range(max_trg_len):\n",
    "            topv, topi = output[ot].topk(1)\n",
    "            \n",
    "            if topi[0].item() == EOS_token:\n",
    "                decoded_words.append(\"<EOS>\")\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(vocab_answers.index2word[topi[0].item()])\n",
    "    \n",
    "    return decoded_words\n",
    "\n",
    "def evaluateRandomly(model, pairs=pairs, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print(f\"source: {pair[0]}\")\n",
    "        print(f\"target: {pair[1]}\")\n",
    "        try:\n",
    "            output_words = evaluate(model, pair[0])\n",
    "            output_sentence = \" \".join(output_words)\n",
    "            print(\"predicted: {}\".format(output_sentence))\n",
    "        except KeyError:\n",
    "            print(\"Word not found in vocabulary\")\n",
    "        \n",
    "def evaluateInput(model, input_sentence):\n",
    "    input_sentence = input_sentence.lower()\n",
    "    output_words = evaluate(model, input_sentence)\n",
    "    output_sentence = \" \".join(output_words)\n",
    "    print(\"predicted: {}\".format(output_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: when were video made avail through the itun store\n",
      "target: octob 12 2005\n",
      "predicted: <SOS> 12 2005 <EOS>\n",
      "source: chimamanda ngozi adichi vompar lee to whom\n",
      "target: william faulkner\n",
      "predicted: <SOS> faulkner <EOS>\n",
      "source: what edict did the ming issu\n",
      "target: imperi edict\n",
      "predicted: <SOS> edict <EOS>\n",
      "source: when did the austrian film of spectr finish\n",
      "target: februari 2015\n",
      "predicted: <SOS> 2015 <EOS>\n",
      "source: who was station along the boarder\n",
      "target: russian border troop\n",
      "predicted: <SOS> border troop <EOS>\n",
      "source: what year did the yongl emperor reign\n",
      "target: 1402 1424\n",
      "predicted: <SOS> 1424 <EOS>\n",
      "source: who are the deputi prime minist\n",
      "target: murodali alimardon and ruqiya qurbanova\n",
      "predicted: <SOS> alimardon and ruqiya qurbanova <EOS>\n",
      "source: how mani borough doe new york citi have\n",
      "target: five\n",
      "predicted: <SOS> <EOS>\n",
      "source: which three person from previous film appear in spectr\n",
      "target: m q and eve moneypenni\n",
      "predicted: <SOS> q and eve moneypenni <EOS>\n",
      "source: who is the develop of eddi street common\n",
      "target: kite realti\n",
      "predicted: <SOS> realti <EOS>\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
