{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Downloads__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchdata\n",
      "  Downloading torchdata-0.5.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.6 MB 4.8 MB/s eta 0:00:01     |██▍                             | 348 kB 4.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchdata) (2.23.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.7/site-packages (from torchdata) (1.25.7)\n",
      "Collecting torch==1.13.1\n",
      "  Downloading torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 887.5 MB 5.5 kB/s  eta 0:00:01     |██████████████████████████████▋ | 849.5 MB 51.4 MB/s eta 0:00:01��███████████▊| 878.5 MB 51.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting portalocker>=2.0.0\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata) (3.0.4)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\"\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.0 MB 38.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[K     |████████████████████████████████| 849 kB 36.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.13.1->torchdata) (3.7.4.1)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\"\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 317.1 MB 23 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\"\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 557.1 MB 5.8 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"->torch==1.13.1->torchdata) (0.34.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"->torch==1.13.1->torchdata) (45.2.0.post20200209)\n",
      "\u001b[31mERROR: torchvision 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.13.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: torchtext 0.12.0 has requirement torch==1.11.0, but you'll have torch 1.13.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: nvidia-cuda-nvrtc-cu11, nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, portalocker, torchdata\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 portalocker-2.7.0 torch-1.13.1 torchdata-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting Cython\n",
      "  Downloading Cython-0.29.33-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 4.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: Cython\n",
      "\u001b[33m  WARNING: The scripts cygdb, cython and cythonize are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed Cython-0.29.33\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "\u001b[31mERROR: torchvision 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.13.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: torchtext 0.12.0 has requirement torch==1.11.0, but you'll have torch 1.13.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: typing-extensions\n",
      "Successfully installed typing-extensions-4.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already up-to-date: torch in /root/.local/lib/python3.7/site-packages (1.13.1)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" in /root/.local/lib/python3.7/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" in /root/.local/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" in /root/.local/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" in /root/.local/lib/python3.7/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /root/.local/lib/python3.7/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\"->torch) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\"->torch) (45.2.0.post20200209)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchtext==0.9.0\n",
      "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.9.0) (4.43.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext==0.9.0) (2.23.0)\n",
      "Collecting torch==1.8.0\n",
      "  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 735.5 MB 9.0 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext==0.9.0) (1.21.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (1.25.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (2019.11.28)\n",
      "Requirement already satisfied: typing-extensions in /root/.local/lib/python3.7/site-packages (from torch==1.8.0->torchtext==0.9.0) (4.4.0)\n",
      "\u001b[31mERROR: torchdata 0.5.1 has requirement torch==1.13.1, but you'll have torch 1.8.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: torchvision 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.8.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: torch, torchtext\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.13.1\n",
      "    Uninstalling torch-1.13.1:\n",
      "      Successfully uninstalled torch-1.13.1\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx and convert-onnx-to-caffe2 are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed torch-1.8.0 torchtext-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchdata\n",
    "%pip install Cython\n",
    "%pip install typing-extensions --upgrade\n",
    "%pip install torch --upgrade\n",
    "%pip install torchtext==0.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Restart the kernel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "\n",
    "import torchtext\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset download\n",
    "train, test = torchtext.datasets.SQuAD1(\"root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Squad 1 dataset contains 100k+ questions and answers based on over 500 articles. More information can be found [here](https://rajpurkar.github.io/SQuAD-explorer/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data rows: 87599\n",
      "Number of test data rows: 10570\n"
     ]
    }
   ],
   "source": [
    "# check number of rows\n",
    "print(f\"Number of training data rows: {train.num_lines}\")\n",
    "print(f\"Number of test data rows: {test.num_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Answer: Saint Bernadette Soubirous\n"
     ]
    }
   ],
   "source": [
    "# print out example question\n",
    "for context, question, answer, answer_start in train:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer[0]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data into dataframe\n",
    "def convert_to_df(iterator):\n",
    "\n",
    "    contexts, questions, answers, answer_starts = [], [], [], []\n",
    "\n",
    "    for line in iterator:\n",
    "        context, question, answer, answer_start = line\n",
    "\n",
    "\n",
    "        contexts.append(context)\n",
    "        questions.append(question)\n",
    "        answers.append(answer[0])\n",
    "        answer_starts.append(answer_start[0])\n",
    "\n",
    "    data_dict = {\n",
    "        \"context\": contexts,\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"answer_start\": answer_starts\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_train = convert_to_df(train)\n",
    "df_test = convert_to_df(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Data Checks_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 87598 entries, 0 to 87597\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   context       87598 non-null  object\n",
      " 1   question      87598 non-null  object\n",
      " 2   answer        87598 non-null  object\n",
      " 3   answer_start  87598 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 2.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73696</th>\n",
       "      <td>The small landowner-cultivators formed the maj...</td>\n",
       "      <td>What type of housing did the Han government pr...</td>\n",
       "      <td>temporary</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67311</th>\n",
       "      <td>Before the French and Indian War, the Appalach...</td>\n",
       "      <td>What did the backcountry settlers want to secure?</td>\n",
       "      <td>their settlement of Kentucky</td>\n",
       "      <td>1059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23553</th>\n",
       "      <td>In military affairs, the use of infantry with ...</td>\n",
       "      <td>Against whom was gunpowder used in 1304?</td>\n",
       "      <td>the Scots</td>\n",
       "      <td>629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3917</th>\n",
       "      <td>The Battle of Long Island, the largest battle ...</td>\n",
       "      <td>When did the English army start to retreat and...</td>\n",
       "      <td>1783</td>\n",
       "      <td>675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27164</th>\n",
       "      <td>Computers control functions at many utilities,...</td>\n",
       "      <td>How did the Stuxnet worm infect industrial equ...</td>\n",
       "      <td>via removable media</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43748</th>\n",
       "      <td>In 1866, the feud between Austria and Prussia ...</td>\n",
       "      <td>What was Germany going to be called if Prussia...</td>\n",
       "      <td>Little Germany</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65463</th>\n",
       "      <td>The population of Paris in its administrative ...</td>\n",
       "      <td>Who created The Paris Urban Area?</td>\n",
       "      <td>INSEE</td>\n",
       "      <td>534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13397</th>\n",
       "      <td>The Government was known officially as the Cou...</td>\n",
       "      <td>What was the RSFSR government called starting ...</td>\n",
       "      <td>Council of Ministers</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6445</th>\n",
       "      <td>Gautama was now determined to complete his spi...</td>\n",
       "      <td>How old was the Buddha at the time of his death?</td>\n",
       "      <td>80</td>\n",
       "      <td>713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86247</th>\n",
       "      <td>Bronx gang life was depicted in the 1974 novel...</td>\n",
       "      <td>When was 'A Bronx Tale' released?</td>\n",
       "      <td>1993</td>\n",
       "      <td>442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 context  \\\n",
       "73696  The small landowner-cultivators formed the maj...   \n",
       "67311  Before the French and Indian War, the Appalach...   \n",
       "23553  In military affairs, the use of infantry with ...   \n",
       "3917   The Battle of Long Island, the largest battle ...   \n",
       "27164  Computers control functions at many utilities,...   \n",
       "43748  In 1866, the feud between Austria and Prussia ...   \n",
       "65463  The population of Paris in its administrative ...   \n",
       "13397  The Government was known officially as the Cou...   \n",
       "6445   Gautama was now determined to complete his spi...   \n",
       "86247  Bronx gang life was depicted in the 1974 novel...   \n",
       "\n",
       "                                                question  \\\n",
       "73696  What type of housing did the Han government pr...   \n",
       "67311  What did the backcountry settlers want to secure?   \n",
       "23553           Against whom was gunpowder used in 1304?   \n",
       "3917   When did the English army start to retreat and...   \n",
       "27164  How did the Stuxnet worm infect industrial equ...   \n",
       "43748  What was Germany going to be called if Prussia...   \n",
       "65463                  Who created The Paris Urban Area?   \n",
       "13397  What was the RSFSR government called starting ...   \n",
       "6445    How old was the Buddha at the time of his death?   \n",
       "86247                  When was 'A Bronx Tale' released?   \n",
       "\n",
       "                             answer  answer_start  \n",
       "73696                     temporary           472  \n",
       "67311  their settlement of Kentucky          1059  \n",
       "23553                     the Scots           629  \n",
       "3917                           1783           675  \n",
       "27164           via removable media           530  \n",
       "43748                Little Germany           503  \n",
       "65463                         INSEE           534  \n",
       "13397          Council of Ministers            87  \n",
       "6445                             80           713  \n",
       "86247                          1993           442  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a feel for what the data looks like\n",
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Build a Vocab__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\" This vocabulary class cleans and indexes words.\n",
    "    \"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.index2word = {0: \"<SOS>\", 1: \"<EOS>\"}\n",
    "        self.word2index = {\"<SOS>\": 0, \"<EOS>\": 1}\n",
    "        self.word2count = {}\n",
    "        self.count = 2 # count SOS and EOS\n",
    "    \n",
    "    # Clean words before adding them to vocab object\n",
    "    def cleanText(self, text):\n",
    "        return prepare_text(text)\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "    \n",
    "    # Index words in our vocabulary\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index: # if word not in index\n",
    "            self.word2index[word] = self.count # add word and word no to words dictionary\n",
    "            self.index2word[self.count] = word # add word to index\n",
    "            self.word2count[word] = 1 # initialise word count\n",
    "            self.count +=1\n",
    "            return True\n",
    "        else:\n",
    "            self.word2count[word] += 1 # increment word count\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "    s = s.lower().strip() # convert to lower, remove excess spaces\n",
    "    s = re.sub(r\"[^a-zA-Z.!?0-9]+\", r\" \", s) # remove all non-letter characters\n",
    "    return s\n",
    "\n",
    "pairs_train = [[normalizeString(q), normalizeString(a)] \n",
    "               for i, (q, a) in df_train[[\"question\", \"answer\"]].iterrows()]\n",
    "pairs_test = [[normalizeString(q), normalizeString(a)] \n",
    "               for i, (q, a) in df_test[[\"question\", \"answer\"]].iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what union are yale s clerical and technical employees a part of?', 'local 34 of unite here']\n"
     ]
    }
   ],
   "source": [
    "# print random sentence and answer\n",
    "random_pair = random.choice(pairs_train)\n",
    "print(random_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and populate vocab object\n",
    "vocab = Vocab(\"squad_1\")\n",
    "for question, answer in pairs_train:\n",
    "    vocab.addSentence(question)\n",
    "    vocab.addSentence(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 66781 words\n"
     ]
    }
   ],
   "source": [
    "# check words added\n",
    "print(f\"Added {vocab.count} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def indexesFromSentence(sentence, vocab=vocab):\n",
    "    # returns a list of indices representing the input sentence\n",
    "    return [vocab.word2index[word] for word in sentence.split(\" \")]\n",
    "\n",
    "def tensorFromSentence(sentence, vocab=vocab):\n",
    "    # appends a EOS token and returns a tensor list of indices representing the input sentence\n",
    "    indexes = indexesFromSentence(sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    # view(-1, 1) specifies that we want the shape of 1 column and whatever number of rows to achieve that shape\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    # combines the function above and returns question and answer tensors\n",
    "    question_tensor = tensorFromSentence(pair[0])\n",
    "    answer_tensor = tensorFromSentence(pair[1])\n",
    "    return (question_tensor, answer_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what union are yale s clerical and technical employees a part of?', 'local 34 of unite here']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[    2],\n",
       "         [  693],\n",
       "         [   60],\n",
       "         [  535],\n",
       "         [   48],\n",
       "         [36923],\n",
       "         [   30],\n",
       "         [14776],\n",
       "         [ 6390],\n",
       "         [   12],\n",
       "         [  383],\n",
       "         [  848],\n",
       "         [    1]], device='cuda:0'),\n",
       " tensor([[ 4354],\n",
       "         [ 7789],\n",
       "         [    6],\n",
       "         [14456],\n",
       "         [ 2629],\n",
       "         [    1]], device='cuda:0'))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test functions\n",
    "print(random_pair)\n",
    "tensorsFromPair(random_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Download the gensim embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "glove_vectors = gensim.downloader.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Explore embedding model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = glove_vectors.vectors.shape[1]\n",
    "print(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.get_index(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6998663544654846),\n",
       " ('person', 0.6443442106246948),\n",
       " ('boy', 0.6208277940750122),\n",
       " ('he', 0.5926738381385803),\n",
       " ('men', 0.5819568634033203),\n",
       " ('himself', 0.5810033082962036),\n",
       " ('one', 0.5779521465301514),\n",
       " ('another', 0.5721587538719177),\n",
       " ('who', 0.5703631639480591),\n",
       " ('him', 0.5670831203460693)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.29784  , -0.13255  , -0.14505  , -0.22752  , -0.027429 ,\n",
       "        0.11005  , -0.039245 , -0.0089607, -0.18866  , -1.1213   ,\n",
       "        0.34793  , -0.30056  , -0.50103  , -0.031383 , -0.032185 ,\n",
       "        0.018318 , -0.090429 , -0.14427  , -0.14306  , -0.057477 ,\n",
       "       -0.020931 ,  0.56276  , -0.018557 ,  0.15168  , -0.25586  ,\n",
       "       -0.081564 ,  0.2803   , -0.10585  , -0.16777  ,  0.21814  ,\n",
       "       -0.11845  ,  0.56475  , -0.12645  , -0.062461 , -0.68043  ,\n",
       "        0.10507  ,  0.24793  , -0.20249  , -0.30726  ,  0.42815  ,\n",
       "        0.38378  , -0.19371  , -0.075951 , -0.058287 , -0.067195 ,\n",
       "        0.2192   ,  0.56116  , -0.28156  , -0.13705  ,  0.45754  ,\n",
       "       -0.14671  , -0.18562  , -0.074146 ,  0.60737  ,  0.07952  ,\n",
       "        0.41023  ,  0.18377  , -0.08532  ,  0.43795  , -0.34727  ,\n",
       "        0.2077   ,  0.50454  ,  0.40244  ,  0.1095   , -0.48078  ,\n",
       "       -0.22372  , -0.54619  , -0.20782  ,  0.13751  , -0.16206  ,\n",
       "       -0.24835  ,  0.17124  ,  0.037355 ,  0.14547  , -0.056205 ,\n",
       "        0.2644   , -0.38029  ,  0.0029947,  0.051608 ,  0.21807  ,\n",
       "       -0.071087 ,  0.31543  ,  0.24593  ,  0.34444  , -0.36354  ,\n",
       "       -0.12137  , -0.013511 ,  0.049564 ,  0.031386 ,  0.10316  ,\n",
       "       -0.44301  ,  0.11663  , -0.055576 , -0.091434 , -0.070799 ,\n",
       "       -0.017031 ,  0.046024 ,  0.096063 ,  0.10148  , -0.33467  ,\n",
       "       -0.043516 ,  0.38271  ,  0.018411 ,  0.0049479,  0.35587  ,\n",
       "        0.40432  ,  0.4823   ,  0.12416  , -0.051926 , -0.081769 ,\n",
       "       -0.21542  ,  0.71766  , -0.0052553, -0.55428  ,  0.68033  ,\n",
       "        0.20255  , -0.19927  , -0.11563  ,  0.0030103, -0.74399  ,\n",
       "       -0.21086  ,  0.069584 , -0.20315  , -0.17027  , -0.11292  ,\n",
       "       -0.044594 , -0.2145   ,  0.26097  ,  0.36969  , -0.65155  ,\n",
       "        0.048649 ,  0.044304 , -0.26604  ,  0.23976  , -0.38281  ,\n",
       "       -0.02353  , -0.020519 , -0.52202  , -0.30595  ,  0.088525 ,\n",
       "        0.28395  ,  0.15056  ,  0.23202  ,  0.30538  , -0.53674  ,\n",
       "        0.21362  ,  0.10999  ,  0.4685   , -0.60017  , -0.13141  ,\n",
       "        0.18707  ,  0.11636  ,  0.24703  ,  0.17168  ,  0.40658  ,\n",
       "       -0.19039  ,  0.033278 , -0.26374  ,  0.28722  ,  0.27659  ,\n",
       "        0.30153  , -0.12622  ,  0.5225   ,  0.13887  , -0.15012  ,\n",
       "       -0.0038264, -0.074287 ,  0.46348  ,  0.26215  , -0.62671  ,\n",
       "        0.21841  , -0.26531  , -0.75294  , -0.36826  ,  0.072989 ,\n",
       "       -0.071807 ,  0.10612  , -0.087064 , -0.053871 ,  0.22379  ,\n",
       "        0.38538  ,  0.053972 ,  0.21208  ,  0.39662  , -0.38609  ,\n",
       "       -0.53     , -0.34169  ,  0.067624 ,  0.11835  ,  0.60372  ,\n",
       "        0.21744  , -0.12806  ,  0.13802  ,  0.22463  , -0.21566  ,\n",
       "       -0.25167  , -0.44859  , -0.39111  , -0.5259   ,  0.21266  ,\n",
       "        1.9729   ,  0.13851  ,  0.32966  ,  0.047626 , -0.37787  ,\n",
       "       -0.31769  , -0.1283   ,  0.34396  , -0.11061  , -0.23346  ,\n",
       "       -0.45134  ,  0.1605   , -0.32801  ,  0.033373 ,  0.42571  ,\n",
       "        0.42283  , -0.16932  ,  0.14068  ,  0.14087  ,  0.20856  ,\n",
       "       -0.24912  ,  0.38337  ,  0.43722  , -0.090575 ,  0.021208 ,\n",
       "       -0.22334  , -0.42596  , -0.47252  , -0.1995   ,  0.32459  ,\n",
       "        0.39765  , -0.091573 , -0.073528 , -0.20074  ,  0.22083  ,\n",
       "        0.011314 ,  0.11977  , -0.14888  , -0.27152  ,  0.28908  ,\n",
       "        0.20407  ,  0.098377 , -0.1424   , -0.059699 ,  0.032529 ,\n",
       "       -0.1877   ,  0.24643  , -0.1128   ,  0.14666  ,  0.076001 ,\n",
       "        0.31109  , -0.22119  ,  0.033051 ,  0.066731 ,  1.5449   ,\n",
       "       -0.11096  ,  0.26438  ,  0.1791   ,  0.039628 , -0.46497  ,\n",
       "        0.40052  , -0.057004 ,  0.11391  ,  0.42397  , -0.57411  ,\n",
       "        0.17205  ,  0.44401  , -0.030134 , -0.22061  ,  0.47222  ,\n",
       "       -0.12734  , -0.12288  ,  0.33832  ,  0.41698  ,  0.18355  ,\n",
       "        0.093473 , -1.9373   ,  0.053236 , -0.10361  ,  0.056681 ,\n",
       "       -0.12726  , -0.11681  ,  0.55039  ,  0.33644  ,  0.27706  ,\n",
       "        0.71184  , -0.1304   ,  0.26108  , -0.2481   ,  0.0032735,\n",
       "       -0.16505  ,  0.11827  , -0.26322  , -0.10169  ,  0.24275  ,\n",
       "        0.16927  , -0.15809  ,  0.27613  , -0.2832   ,  0.50554  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors[\"man\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Create weights matrix to map each word embedding to the word in the vocab_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_len = len(vocab.word2index) # length of vocab\n",
    "weights_matrix = np.zeros((matrix_len, embedding_dim)) # initialise empty weights matrix\n",
    "words_found = 0\n",
    "\n",
    "# add words to weights matrix\n",
    "for i, word in enumerate(vocab.word2index):\n",
    "    try:\n",
    "        weights_matrix[i] = glove_vectors[word] # map to glove vector word embedding\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim, )) # insert random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "weights_matrix[vocab.word2index[\"man\"]] - glove_vectors[\"man\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define Model Structure__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQLTP2Wmi1eB"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        # copy loaded weights matrix into embedding weights - a vector representation of the model word inputs\n",
    "        #self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        # initialise lstm to take input dimension of embedding size and output hidden dimension\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers=num_layers)\n",
    "        \n",
    "    \n",
    "    def forward(self, src):\n",
    "        \n",
    "        '''\n",
    "        Inputs: src, the src vector\n",
    "        Outputs: output, the encoder outputs\n",
    "                hidden, the hidden state\n",
    "                cell, the cell state\n",
    "        '''\n",
    "        # src shape: (seq_length, batch_size)\n",
    "        embedding = self.embedding(src).view(1, 1, -1)\n",
    "        # embedding shape: (seq_length, batch_size, embedding_size)\n",
    "        output, (hidden, cell) = self.lstm(embedding) # output, hidden and cell state\n",
    "        \n",
    "        return hidden, cell\n",
    "    \n",
    "    def initHidden(self):# initialise zero tensor with shape (1, 1, hidden_size)\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "      \n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size # length of vocab\n",
    "        self.num_layers = num_layers        \n",
    "        \n",
    "        # self.embedding provides a vector representation of the target to our model\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        #self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        # self.lstm, accepts the embeddings and outputs a hidden state\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers=num_layers)\n",
    "        # self.output, predicts on the hidden state via a linear output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, trg, hidden, cell):\n",
    "        \n",
    "        '''\n",
    "        Inputs: trg, the target vector\n",
    "        Outputs: predictions, the predictions\n",
    "                hidden, the hidden state\n",
    "                cell, the cell state\n",
    "        '''\n",
    "        # shape of src is batch_size, but we want (1, batch_size)\n",
    "        # we want batch_size batches of a single word at a time\n",
    "        trg = trg.unsqueeze(0) # add one dimension\n",
    "        embedding = self.embedding(trg).view(1, 1, -1)\n",
    "        # embedding shape: (1, batch_size, embedding_size)\n",
    "        output, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
    "        # shape of outputs: (1, batch_size, hidden_size)\n",
    "        predictions = self.softmax(self.fc(output[0]))\n",
    "        # shape of predictions: (1, batch_size, length_of_vocab)        \n",
    "        \n",
    "        return predictions, hidden, cell\n",
    "    \n",
    "    def initHidden(self): # initialise zero tensor with shape (1, 1, hidden_size)\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "        \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device=device\n",
    "    \n",
    "    # teacher forcing ratio - switch between using target and prediction words as inputs for next word\n",
    "    def forward(self, src, trg, max_trg_len=100, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = src.shape[1]\n",
    "        \n",
    "        input_length = src.size(0) # how many question words\n",
    "        target_length = trg.size(0) if self.training else max_trg_len# how many answer words\n",
    "        \n",
    "        target_vocab_size = len(vocab.word2index)\n",
    "        \n",
    "        outputs = torch.zeros(target_length, batch_size, target_vocab_size).to(self.device)\n",
    "        \n",
    "        for i in range(input_length):\n",
    "            encoder_hidden, cell = self.encoder(src[i]) # process and encode the entire question\n",
    "            \n",
    "        decoder_hidden = encoder_hidden # initialise decoder hidden state with encoder hidden state\n",
    "        decoder_input = torch.LongTensor([[SOS_token]]).to(device) # add SOS token as first predicted word\n",
    "        \n",
    "        for t in range(1, target_length):\n",
    "            output, decoder_hidden, cell = self.decoder(decoder_input, decoder_hidden, cell)\n",
    "            \n",
    "            outputs[t] = output # add decoder predictions array to outputs\n",
    "            # (batch_size, vocab_size)\n",
    "            best_guess = output.argmax(1) # get index of best word guess\n",
    "            teacher_force = False # initialise\n",
    "            if self.training:\n",
    "                # use target word if teacher forcing, else use word with the highest predicted value\n",
    "                teacher_force = random.random() < teacher_forcing_ratio # update where relevant\n",
    "                decoder_input = trg[t] if teacher_force else best_guess \n",
    "            else:\n",
    "                decoder_input = best_guess\n",
    "                \n",
    "            if (teacher_force == False and decoder_input.item() == EOS_token):\n",
    "                break\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 10])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.zeros(target_length, batch_size, target_vocab_size)\n",
    "b = torch.zeros(10, 1, 10)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 10])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[1:].view(-1, b.shape[-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor([[SOS_token]]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def showPlot(points):\n",
    "    # This function plots the input points and sets defined tick intervals on the y axis\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    loc = ticker.MultipleLocator(base=0.2) # define tick intervals\n",
    "    ax.yaxis.set_major_locator(loc) # set y axis tick intervals\n",
    "    plt.plot(points)\n",
    "    \n",
    "def asMinutes(s):\n",
    "    # format seconds as minutes\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    # calculate the time between now and since\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (- %s)\" % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcModel(model, input_tensor, target_tensor, model_optimizer, criterion):\n",
    "    model_optimizer.zero_grad() # don't accumulate gradient\n",
    "    \n",
    "    loss = 0\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    output = model(input_tensor, target_tensor)\n",
    "    \n",
    "    num_iter = output.size(0) # number of predicted words\n",
    "    \n",
    "    # calculate loss from predicted sentence with expected result\n",
    "    for ot in range(num_iter):\n",
    "        loss += criterion(output[ot], target_tensor[ot])\n",
    "        \n",
    "    loss.backward() # calculate gradients\n",
    "    model_optimizer.step() # update weights\n",
    "    epoch_loss = loss.item() / num_iter # avg loss\n",
    "    \n",
    "    return epoch_loss\n",
    "\n",
    "def trainModel(model, pairs, num_iterations=10000, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss() # crossentropyloss = softmax + NLLLoss\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0 # reset every print_every\n",
    "    plot_loss_total = 0 # reset every plot_every\n",
    "    \n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs)) for _ in range(num_iterations)]\n",
    "    \n",
    "    for iter_ in range(1, num_iterations+1):\n",
    "        training_pair = training_pairs[iter_ - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        \n",
    "        loss = calcModel(model, input_tensor, target_tensor, optimizer, criterion)\n",
    "        \n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss        \n",
    "        \n",
    "        if iter_ % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(\"{0:s} ({1:d} {2:.0f}%), {3:.4f}\". format(timeSince(start, iter_ / num_iterations),\n",
    "                                                 iter_, iter_ / num_iterations * 100, print_loss_avg))\n",
    "            \n",
    "        if iter_ % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "            \n",
    "    showPlot(plot_losses)\n",
    "            \n",
    "    torch.save(model.state_dict(), \"mytraining.pt\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "vocab_length = len(vocab.word2index)\n",
    "input_size_enc = vocab_length\n",
    "input_size_dec = vocab_length\n",
    "output_size = vocab_length\n",
    "enc_embed_size = embedding_dim\n",
    "dec_embed_size = embedding_dim\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "# initialise models\n",
    "encoder = Encoder(input_size_enc, enc_embed_size, hidden_size, num_layers=num_layers)\n",
    "decoder = Decoder(input_size_dec, dec_embed_size, hidden_size, output_size, num_layers=num_layers)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-a30e819cbf0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-2065b6b3233d>\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(model, pairs, num_iterations, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalcModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-2065b6b3233d>\u001b[0m in \u001b[0;36mcalcModel\u001b[0;34m(model, input_tensor, target_tensor, model_optimizer, criterion)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnum_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# number of predicted words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-d8b696d18f05>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, max_trg_len, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;31m# add decoder predictions array to outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-d8b696d18f05>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, trg, hidden, cell)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# we want batch_size batches of a single word at a time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add one dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;31m# embedding shape: (1, batch_size, embedding_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    145\u001b[0m         return F.embedding(\n\u001b[1;32m    146\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1911\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1913\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model_1 = trainModel(model, pairs_train, learning_rate=learning_rate, num_iterations=num_iterations, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  379],\n",
       "         [ 1382],\n",
       "         [   21],\n",
       "         [  785],\n",
       "         [    7],\n",
       "         [ 3882],\n",
       "         [30209],\n",
       "         [12375],\n",
       "         [   92],\n",
       "         [14637],\n",
       "         [    4],\n",
       "         [35020],\n",
       "         [    1]], device='cuda:0'),\n",
       " tensor([[48967],\n",
       "         [49067],\n",
       "         [    1]], device='cuda:0'))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tensorsFromPair(random.choice(pairs_train))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model_1(test[0], test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, source, max_trg_len=100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(source)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        \n",
    "        decoded_words = []\n",
    "        \n",
    "        output = model(input_tensor, None)\n",
    "        \n",
    "        for ot in range(max_trg_len):\n",
    "            topv, topi = output[ot].topk(1)\n",
    "            \n",
    "            if topi[0].item() == EOS_token:\n",
    "                decoded_words.append(\"<EOS>\")\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(vocab.index2word[topi[0].item()])\n",
    "    \n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(model, pairs=pairs_test, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print(f\"source: {pair[0]}\")\n",
    "        print(f\"target: {pair[1]}\")\n",
    "        \n",
    "        output_words = evaluate(model, pair)\n",
    "        output_sentence = \" \".join(output_words)\n",
    "        print(\"predicted {}\".format(output_sentence))\n",
    "        \n",
    "def evaluateInput(model, input_sentence):\n",
    "    \n",
    "    output_words = evaluate(model, input_sentence)\n",
    "    output_sentence = \" \".join(output_words)\n",
    "    print(\"predicted {}\".format(output_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted SOS <EOS>\n"
     ]
    }
   ],
   "source": [
    "sent = \"when did the queen die?\"\n",
    "evaluateInput(model_1, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  38],\n",
       "        [  39],\n",
       "        [   7],\n",
       "        [1786],\n",
       "        [4036],\n",
       "        [   1]], device='cuda:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorFromSentence(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
