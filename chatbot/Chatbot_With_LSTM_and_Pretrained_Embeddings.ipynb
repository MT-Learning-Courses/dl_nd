{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Downloads__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchdata\n",
      "  Downloading torchdata-0.5.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.6 MB 4.8 MB/s eta 0:00:01     |██▍                             | 348 kB 4.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchdata) (2.23.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.7/site-packages (from torchdata) (1.25.7)\n",
      "Collecting torch==1.13.1\n",
      "  Downloading torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 887.5 MB 5.5 kB/s  eta 0:00:01     |██████████████████████████████▋ | 849.5 MB 51.4 MB/s eta 0:00:01��███████████▊| 878.5 MB 51.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting portalocker>=2.0.0\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata) (3.0.4)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\"\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.0 MB 38.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[K     |████████████████████████████████| 849 kB 36.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.13.1->torchdata) (3.7.4.1)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\"\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 317.1 MB 23 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\"\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 557.1 MB 5.8 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"->torch==1.13.1->torchdata) (0.34.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"->torch==1.13.1->torchdata) (45.2.0.post20200209)\n",
      "\u001b[31mERROR: torchvision 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.13.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: torchtext 0.12.0 has requirement torch==1.11.0, but you'll have torch 1.13.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: nvidia-cuda-nvrtc-cu11, nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, portalocker, torchdata\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 portalocker-2.7.0 torch-1.13.1 torchdata-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting Cython\n",
      "  Downloading Cython-0.29.33-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 4.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: Cython\n",
      "\u001b[33m  WARNING: The scripts cygdb, cython and cythonize are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed Cython-0.29.33\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "\u001b[31mERROR: torchvision 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.13.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: torchtext 0.12.0 has requirement torch==1.11.0, but you'll have torch 1.13.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: typing-extensions\n",
      "Successfully installed typing-extensions-4.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already up-to-date: torch in /root/.local/lib/python3.7/site-packages (1.13.1)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" in /root/.local/lib/python3.7/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" in /root/.local/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" in /root/.local/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" in /root/.local/lib/python3.7/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /root/.local/lib/python3.7/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\"->torch) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\"->torch) (45.2.0.post20200209)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchtext==0.9.0\n",
      "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.9.0) (4.43.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext==0.9.0) (2.23.0)\n",
      "Collecting torch==1.8.0\n",
      "  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 735.5 MB 9.0 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext==0.9.0) (1.21.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (1.25.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (2019.11.28)\n",
      "Requirement already satisfied: typing-extensions in /root/.local/lib/python3.7/site-packages (from torch==1.8.0->torchtext==0.9.0) (4.4.0)\n",
      "\u001b[31mERROR: torchdata 0.5.1 has requirement torch==1.13.1, but you'll have torch 1.8.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: torchvision 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.8.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: torch, torchtext\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.13.1\n",
      "    Uninstalling torch-1.13.1:\n",
      "      Successfully uninstalled torch-1.13.1\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx and convert-onnx-to-caffe2 are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed torch-1.8.0 torchtext-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchdata\n",
    "%pip install Cython\n",
    "%pip install typing-extensions --upgrade\n",
    "%pip install torch --upgrade\n",
    "%pip install torchtext==0.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Restart the kernel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "\n",
    "import torchtext\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset download\n",
    "train, test = torchtext.datasets.SQuAD1(\"root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Squad 1 dataset contains 100k+ questions and answers based on over 500 articles. More information can be found [here](https://rajpurkar.github.io/SQuAD-explorer/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data rows: 87599\n",
      "Number of test data rows: 10570\n"
     ]
    }
   ],
   "source": [
    "# check number of rows\n",
    "print(f\"Number of training data rows: {train.num_lines}\")\n",
    "print(f\"Number of test data rows: {test.num_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Answer: Saint Bernadette Soubirous\n"
     ]
    }
   ],
   "source": [
    "# print out example question\n",
    "for context, question, answer, answer_start in train:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer[0]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data into dataframe\n",
    "def convert_to_df(iterator):\n",
    "\n",
    "    contexts, questions, answers, answer_starts = [], [], [], []\n",
    "\n",
    "    for line in iterator:\n",
    "        context, question, answer, answer_start = line\n",
    "\n",
    "\n",
    "        contexts.append(context)\n",
    "        questions.append(question)\n",
    "        answers.append(answer[0])\n",
    "        answer_starts.append(answer_start[0])\n",
    "\n",
    "    data_dict = {\n",
    "        \"context\": contexts,\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"answer_start\": answer_starts\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_train = convert_to_df(train)\n",
    "df_test = convert_to_df(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Data Checks_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 87598 entries, 0 to 87597\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   context       87598 non-null  object\n",
      " 1   question      87598 non-null  object\n",
      " 2   answer        87598 non-null  object\n",
      " 3   answer_start  87598 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 2.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74658</th>\n",
       "      <td>The Qing dynasty (1644–1911) was founded after...</td>\n",
       "      <td>What were the Manchus originally named?</td>\n",
       "      <td>Jurchens</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37216</th>\n",
       "      <td>In 1853, Victoria gave birth to her eighth chi...</td>\n",
       "      <td>Who was Victoria's eighth child?</td>\n",
       "      <td>Leopold</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8912</th>\n",
       "      <td>It should be emphasized, however, that for Whi...</td>\n",
       "      <td>In what did Whitehead believe that those conce...</td>\n",
       "      <td>primordial nature of God</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77274</th>\n",
       "      <td>The Gram stain, developed in 1884 by Hans Chri...</td>\n",
       "      <td>What color is typical for Gram-negative bacter...</td>\n",
       "      <td>pink</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34576</th>\n",
       "      <td>A third concern with the Kinsey scale is that ...</td>\n",
       "      <td>What is the advantage of measuring these eleme...</td>\n",
       "      <td>the degree of heterosexual and homosexual can ...</td>\n",
       "      <td>878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20044</th>\n",
       "      <td>Although the Spanish Empire had a residual cla...</td>\n",
       "      <td>What title did the Germans give to Kabua?</td>\n",
       "      <td>King of the Ralik Islands</td>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21706</th>\n",
       "      <td>The first vertebrates appeared over 500 millio...</td>\n",
       "      <td>How long ago did the first vertebrate organism...</td>\n",
       "      <td>over 500 million years ago</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73538</th>\n",
       "      <td>During the turbulent reign of Wang Mang, Han l...</td>\n",
       "      <td>Which Basin did the Han lose authority of?</td>\n",
       "      <td>Tarim</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57349</th>\n",
       "      <td>Lateral-cut disc records were developed in the...</td>\n",
       "      <td>Where is the museum dedicated to Berliner loca...</td>\n",
       "      <td>Montreal</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69412</th>\n",
       "      <td>Any philosophy that assigns crucial importance...</td>\n",
       "      <td>What branch of idealism believes that we can o...</td>\n",
       "      <td>Epistemological</td>\n",
       "      <td>549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 context  \\\n",
       "74658  The Qing dynasty (1644–1911) was founded after...   \n",
       "37216  In 1853, Victoria gave birth to her eighth chi...   \n",
       "8912   It should be emphasized, however, that for Whi...   \n",
       "77274  The Gram stain, developed in 1884 by Hans Chri...   \n",
       "34576  A third concern with the Kinsey scale is that ...   \n",
       "20044  Although the Spanish Empire had a residual cla...   \n",
       "21706  The first vertebrates appeared over 500 millio...   \n",
       "73538  During the turbulent reign of Wang Mang, Han l...   \n",
       "57349  Lateral-cut disc records were developed in the...   \n",
       "69412  Any philosophy that assigns crucial importance...   \n",
       "\n",
       "                                                question  \\\n",
       "74658            What were the Manchus originally named?   \n",
       "37216                   Who was Victoria's eighth child?   \n",
       "8912   In what did Whitehead believe that those conce...   \n",
       "77274  What color is typical for Gram-negative bacter...   \n",
       "34576  What is the advantage of measuring these eleme...   \n",
       "20044          What title did the Germans give to Kabua?   \n",
       "21706  How long ago did the first vertebrate organism...   \n",
       "73538         Which Basin did the Han lose authority of?   \n",
       "57349  Where is the museum dedicated to Berliner loca...   \n",
       "69412  What branch of idealism believes that we can o...   \n",
       "\n",
       "                                                  answer  answer_start  \n",
       "74658                                           Jurchens           154  \n",
       "37216                                            Leopold            50  \n",
       "8912                            primordial nature of God           429  \n",
       "77274                                               pink           272  \n",
       "34576  the degree of heterosexual and homosexual can ...           878  \n",
       "20044                          King of the Ralik Islands           551  \n",
       "21706                         over 500 million years ago            31  \n",
       "73538                                              Tarim            67  \n",
       "57349                                           Montreal          1250  \n",
       "69412                                    Epistemological           549  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a feel for what the data looks like\n",
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Build a Vocab__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\" This vocabulary class cleans and indexes words.\n",
    "    \"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.index2word = {0: \"<SOS>\", 1: \"<EOS>\"}\n",
    "        self.word2index = {\"<SOS>\": 0, \"<EOS>\": 1}\n",
    "        self.word2count = {}\n",
    "        self.count = 2 # count SOS and EOS\n",
    "    \n",
    "    # Clean words before adding them to vocab object\n",
    "    def cleanText(self, text):\n",
    "        return prepare_text(text)\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "    \n",
    "    # Index words in our vocabulary\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index: # if word not in index\n",
    "            self.word2index[word] = self.count # add word and word no to words dictionary\n",
    "            self.index2word[self.count] = word # add word to index\n",
    "            self.word2count[word] = 1 # initialise word count\n",
    "            self.count +=1\n",
    "            return True\n",
    "        else:\n",
    "            self.word2count[word] += 1 # increment word count\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "    s = s.lower().strip() # convert to lower, remove excess spaces\n",
    "    s = re.sub(r\"[^a-zA-Z.!?0-9]+\", r\" \", s) # remove all non-letter characters\n",
    "    return s\n",
    "\n",
    "pairs_train = [[normalizeString(q), normalizeString(a)] \n",
    "               for i, (q, a) in df_train[[\"question\", \"answer\"]].iterrows()]\n",
    "pairs_test = [[normalizeString(q), normalizeString(a)] \n",
    "               for i, (q, a) in df_test[[\"question\", \"answer\"]].iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how are the leads of an axially configured capacitor arranged?', 'on a common axis']\n"
     ]
    }
   ],
   "source": [
    "# print random sentence and answer\n",
    "random_pair = random.choice(pairs_train)\n",
    "print(random_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and populate vocab object\n",
    "vocab = Vocab(\"squad_1\")\n",
    "for question, answer in pairs_train:\n",
    "    vocab.addSentence(question)\n",
    "    vocab.addSentence(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 66781 words\n"
     ]
    }
   ],
   "source": [
    "# check words added\n",
    "print(f\"Added {vocab.count} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def indexesFromSentence(sentence, vocab=vocab):\n",
    "    # returns a list of indices representing the input sentence\n",
    "    return [vocab.word2index[word] for word in sentence.split(\" \")]\n",
    "\n",
    "def tensorFromSentence(sentence, vocab=vocab):\n",
    "    # appends a EOS token and returns a tensor list of indices representing the input sentence\n",
    "    indexes = indexesFromSentence(sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    # view(-1, 1) specifies that we want the shape of 1 column and whatever number of rows to achieve that shape\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    # combines the function above and returns question and answer tensors\n",
    "    question_tensor = tensorFromSentence(pair[0])\n",
    "    answer_tensor = tensorFromSentence(pair[1])\n",
    "    return (question_tensor, answer_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how are the leads of an axially configured capacitor arranged?', 'on a common axis']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[   46],\n",
       "         [   60],\n",
       "         [    7],\n",
       "         [12107],\n",
       "         [    6],\n",
       "         [  157],\n",
       "         [49322],\n",
       "         [33951],\n",
       "         [49245],\n",
       "         [26494],\n",
       "         [    1]], device='cuda:0'),\n",
       " tensor([[   33],\n",
       "         [   12],\n",
       "         [   64],\n",
       "         [10382],\n",
       "         [    1]], device='cuda:0'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test functions\n",
    "print(random_pair)\n",
    "tensorsFromPair(random_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Download the gensim embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "source": [
    "glove_vectors = gensim.downloader.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Explore embedding model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = glove_vectors.vectors.shape[1]\n",
    "print(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.get_index(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.8860337734222412),\n",
       " ('boy', 0.8564431071281433),\n",
       " ('another', 0.8452839851379395),\n",
       " ('old', 0.8372183442115784),\n",
       " ('one', 0.827606201171875),\n",
       " ('who', 0.8244695663452148),\n",
       " ('him', 0.8194693922996521),\n",
       " ('turned', 0.8154467940330505),\n",
       " ('whose', 0.811974048614502),\n",
       " ('himself', 0.807725727558136)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.094386,  0.43007 , -0.17224 , -0.45529 ,  1.6447  ,  0.40335 ,\n",
       "       -0.37263 ,  0.25071 , -0.10588 ,  0.10778 , -0.10848 ,  0.15181 ,\n",
       "       -0.65396 ,  0.55054 ,  0.59591 , -0.46278 ,  0.11847 ,  0.64448 ,\n",
       "       -0.70948 ,  0.23947 , -0.82905 ,  1.272   ,  0.033021,  0.2935  ,\n",
       "        0.3911  , -2.8094  , -0.70745 ,  0.4106  ,  0.3894  , -0.2913  ,\n",
       "        2.6124  , -0.34576 , -0.16832 ,  0.25154 ,  0.31216 ,  0.31639 ,\n",
       "        0.12539 , -0.012646,  0.22297 , -0.56585 , -0.086264,  0.62549 ,\n",
       "       -0.0576  ,  0.29375 ,  0.66005 , -0.53115 , -0.48233 , -0.97925 ,\n",
       "        0.53135 , -0.11725 ], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors[\"man\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Create weights matrix to map each word embedding to the word in the vocab_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_len = len(vocab.word2index) # length of vocab\n",
    "weights_matrix = np.zeros((matrix_len, embedding_dim)) # initialise empty weights matrix\n",
    "words_found = 0\n",
    "\n",
    "# add words to weights matrix\n",
    "for i, word in enumerate(vocab.word2index):\n",
    "    try:\n",
    "        weights_matrix[i] = glove_vectors[word] # map to glove vector word embedding\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim, )) # insert random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "weights_matrix[vocab.word2index[\"man\"]] - glove_vectors[\"man\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define Model Structure__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "oQLTP2Wmi1eB"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        # copy loaded weights matrix into embedding weights - a vector representation of the model word inputs\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        # initialise lstm to take input dimension of embedding size and output hidden dimension\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers=num_layers)\n",
    "        \n",
    "    \n",
    "    def forward(self, src):\n",
    "        \n",
    "        '''\n",
    "        Inputs: src, the src vector\n",
    "        Outputs: output, the encoder outputs\n",
    "                hidden, the hidden state\n",
    "                cell, the cell state\n",
    "        '''\n",
    "        # src shape: (seq_length, batch_size)\n",
    "        embedding = self.embedding(src).view(1, 1, -1)\n",
    "        # embedding shape: (seq_length, batch_size, embedding_size)\n",
    "        output, (hidden, cell) = self.lstm(embedding) # output, hidden and cell state\n",
    "        \n",
    "        return hidden, cell\n",
    "    \n",
    "    def initHidden(self):# initialise zero tensor with shape (1, 1, hidden_size)\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "      \n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size # length of vocab\n",
    "        self.num_layers = num_layers        \n",
    "        \n",
    "        # self.embedding provides a vector representation of the target to our model\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        # self.lstm, accepts the embeddings and outputs a hidden state\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers=num_layers)\n",
    "        # self.output, predicts on the hidden state via a linear output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, trg, hidden, cell):\n",
    "        \n",
    "        '''\n",
    "        Inputs: trg, the target vector\n",
    "        Outputs: predictions, the predictions\n",
    "                hidden, the hidden state\n",
    "                cell, the cell state\n",
    "        '''\n",
    "        # shape of src is batch_size, but we want (1, batch_size)\n",
    "        # we want batch_size batches of a single word at a time\n",
    "        trg = trg.unsqueeze(0) # add one dimension\n",
    "        embedding = self.embedding(trg).view(1, 1, -1)\n",
    "        # embedding shape: (1, batch_size, embedding_size)\n",
    "        output, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
    "        # shape of outputs: (1, batch_size, hidden_size)\n",
    "        predictions = self.softmax(self.fc(output[0]))\n",
    "        # shape of predictions: (1, batch_size, length_of_vocab)        \n",
    "        \n",
    "        return predictions, hidden, cell\n",
    "    \n",
    "    def initHidden(self): # initialise zero tensor with shape (1, 1, hidden_size)\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "        \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device=device\n",
    "    \n",
    "    # teacher forcing ratio - switch between using target and prediction words as inputs for next word\n",
    "    def forward(self, src, trg, max_trg_len=100, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = src.shape[1]\n",
    "        \n",
    "        input_length = src.size(0) # how many question words\n",
    "        target_length = trg.size(0) if self.training else max_trg_len# how many answer words\n",
    "        \n",
    "        target_vocab_size = len(vocab.word2index)\n",
    "        \n",
    "        outputs = torch.zeros(target_length, batch_size, target_vocab_size).to(self.device)\n",
    "        \n",
    "        for i in range(input_length):\n",
    "            encoder_hidden, cell = self.encoder(src[i]) # process and encode the entire question\n",
    "            \n",
    "        decoder_hidden = encoder_hidden # initialise decoder hidden state with encoder hidden state\n",
    "        decoder_input = torch.LongTensor([[SOS_token]]).to(device) # add SOS token as first predicted word\n",
    "        \n",
    "        for t in range(1, target_length):\n",
    "            output, decoder_hidden, cell = self.decoder(decoder_input, decoder_hidden, cell)\n",
    "            \n",
    "            outputs[t] = output # add decoder predictions array to outputs\n",
    "            # (batch_size, vocab_size)\n",
    "            best_guess = output.argmax(1) # get index of best word guess\n",
    "            teacher_force = False # initialise\n",
    "            if self.training:\n",
    "                # use target word if teacher forcing, else use word with the highest predicted value\n",
    "                teacher_force = random.random() < teacher_forcing_ratio # update where relevant\n",
    "                decoder_input = trg[t] if teacher_force else best_guess \n",
    "            else:\n",
    "                decoder_input = best_guess\n",
    "                \n",
    "            if (teacher_force == False and decoder_input.item() == EOS_token):\n",
    "                break\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def showPlot(points):\n",
    "    # This function plots the input points and sets defined tick intervals on the y axis\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    loc = ticker.MultipleLocator(base=0.2) # define tick intervals\n",
    "    ax.yaxis.set_major_locator(loc) # set y axis tick intervals\n",
    "    plt.plot(points)\n",
    "    \n",
    "def asMinutes(s):\n",
    "    # format seconds as minutes\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    # calculate the time between now and since\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (- %s)\" % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcModel(model, input_tensor, target_tensor, model_optimizer, criterion):\n",
    "    model_optimizer.zero_grad() # don't accumulate gradient\n",
    "    \n",
    "    loss = 0\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    output = model(input_tensor, target_tensor)\n",
    "    \n",
    "    num_iter = output.size(0) # number of predicted words\n",
    "    \n",
    "    # calculate loss from predicted sentence with expected result\n",
    "    for ot in range(num_iter):\n",
    "        loss += criterion(output[ot], target_tensor[ot])\n",
    "        \n",
    "    loss.backward() # calculate gradients\n",
    "    model_optimizer.step() # update weights\n",
    "    epoch_loss = loss.item() / num_iter # avg loss\n",
    "    \n",
    "    return epoch_loss\n",
    "\n",
    "def trainModel(model, pairs, num_iterations=10000, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss() # crossentropyloss = softmax + NLLLoss\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0 # reset every print_every\n",
    "    plot_loss_total = 0 # reset every plot_every\n",
    "    \n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs)) for _ in range(num_iterations)]\n",
    "    \n",
    "    for iter_ in range(1, num_iterations+1):\n",
    "        training_pair = training_pairs[iter_ - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        \n",
    "        loss = calcModel(model, input_tensor, target_tensor, optimizer, criterion)\n",
    "        \n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss        \n",
    "        \n",
    "        if iter_ % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(\"{0:s} ({1:d} {2:.0f}%), {3:.4f}\". format(timeSince(start, iter_ / num_iterations),\n",
    "                                                 iter_, iter_ / num_iterations * 100, print_loss_avg))\n",
    "            \n",
    "        if iter_ % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "            \n",
    "    showPlot(plot_losses)\n",
    "            \n",
    "    torch.save(model.state_dict(), \"mytraining.pt\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "vocab_length = len(vocab.word2index)\n",
    "input_size_enc = vocab_length\n",
    "input_size_dec = vocab_length\n",
    "output_size = vocab_length\n",
    "enc_embed_size = embedding_dim\n",
    "dec_embed_size = embedding_dim\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "# initialise models\n",
    "encoder = Encoder(input_size_enc, enc_embed_size, hidden_size, num_layers=num_layers)\n",
    "decoder = Decoder(input_size_dec, dec_embed_size, hidden_size, output_size, num_layers=num_layers)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 19s (- 0m 19s) (500 50%), 3.7291\n",
      "0m 39s (- 0m 0s) (1000 100%), 2.6068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train model\n",
    "model_1 = trainModel(model, pairs_train, learning_rate=learning_rate, num_iterations=num_iterations, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, source, max_trg_len=100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(source)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        \n",
    "        decoded_words = []\n",
    "        \n",
    "        output = model(input_tensor, None)\n",
    "        \n",
    "        for ot in range(max_trg_len):\n",
    "            topv, topi = output[ot].topk(1)\n",
    "            \n",
    "            if topi[0].item() == EOS_token:\n",
    "                decoded_words.append(\"<EOS>\")\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(vocab.index2word[topi[0].item()])\n",
    "    \n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(model, pairs=pairs_test, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print(f\"source: {pair[0]}\")\n",
    "        print(f\"target: {pair[1]}\")\n",
    "        \n",
    "        output_words = evaluate(model, pair)\n",
    "        output_sentence = \" \".join(output_words)\n",
    "        print(\"predicted: {}\".format(output_sentence))\n",
    "        \n",
    "def evaluateInput(model, input_sentence):\n",
    "    input_sentence = input_sentence.lower()\n",
    "    output_words = evaluate(model, input_sentence)\n",
    "    output_sentence = \" \".join(output_words)\n",
    "    print(\"predicted: {}\".format(output_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: <SOS> <EOS>\n"
     ]
    }
   ],
   "source": [
    "sent = \"What is the first day of the week?\"\n",
    "evaluateInput(model_1, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  38],\n",
       "        [  39],\n",
       "        [   7],\n",
       "        [1786],\n",
       "        [4036],\n",
       "        [   1]], device='cuda:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorFromSentence(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  379],\n",
       "         [ 1382],\n",
       "         [   21],\n",
       "         [  785],\n",
       "         [    7],\n",
       "         [ 3882],\n",
       "         [30209],\n",
       "         [12375],\n",
       "         [   92],\n",
       "         [14637],\n",
       "         [    4],\n",
       "         [35020],\n",
       "         [    1]], device='cuda:0'),\n",
       " tensor([[48967],\n",
       "         [49067],\n",
       "         [    1]], device='cuda:0'))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tensorsFromPair(random.choice(pairs_train))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model_1(test[0], test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 10])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.zeros(target_length, batch_size, target_vocab_size)\n",
    "b = torch.zeros(10, 1, 10)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 10])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[1:].view(-1, b.shape[-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor([[SOS_token]]).to(device)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
